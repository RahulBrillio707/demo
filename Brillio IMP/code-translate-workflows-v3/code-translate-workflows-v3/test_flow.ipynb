{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf8eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph import MessagesState\n",
    "# from langchain_aws import ChatBedrock,\n",
    "from langchain_openai import ChatOpenAI,AzureChatOpenAI\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.types import Command\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    "    trim_messages,\n",
    ")\n",
    "\n",
    "import mermaid as md\n",
    "import pyparsing as pp\n",
    "import networkx as nx\n",
    "import re \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from typing import List, Dict, Optional, Any\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "import ast\n",
    "import json\n",
    "\n",
    "# Set up Azure OpenAI credentials\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = 'EOpDX9B2g0n6HYPMMTSJCeN00D6leGgpflKvp5UROz4y1vmu5MTmJQQJ99BAACHYHv6XJ3w3AAAAACOGl6ms'\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = 'https://brillioagentsh8218774988.openai.azure.com'\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-4o-2\",\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "\n",
    "# # Import your existing tool implementations:\n",
    "# from react_test_v2 import parse_sas_file, parse_sas_code, analyze_sas_elements\n",
    "# from react_test_v2 import (\n",
    "#     chunk_sas_file,\n",
    "#     chunk_sas_script,\n",
    "#     extract_ast_with_llm,\n",
    "#     translate_sas_chunk,\n",
    "#     merge_and_deduplicate_code,\n",
    "#     validate_python_syntax,\n",
    "#     validate_pyspark_syntax,\n",
    "# )\n",
    "\n",
    "\n",
    "# 1. Define the full custom state\n",
    "@dataclass\n",
    "class SASPipelineState:\n",
    "    # — Inputs —\n",
    "    file_path:       Optional[str]     = None\n",
    "    script_content:  Optional[str]     = None\n",
    "    chunk_size:      int               = 100   # default chunk size\n",
    "\n",
    "    # — Chunking outputs —\n",
    "    chunks:          List[str]         = field(default_factory=list)\n",
    "    element_summary: Dict[str,int]     = field(default_factory=dict)\n",
    "    num_chunks:      int               = 0\n",
    "    script_length:   Optional[int]     = None\n",
    "\n",
    "    # — AST extraction —\n",
    "    chunks_with_ast: List[Dict[str,str]] = field(default_factory=list)\n",
    "\n",
    "    # — Translation outputs —\n",
    "    translated_chunks: List[Dict[str,Any]] = field(default_factory=list)\n",
    "\n",
    "    # — Merge & dedupe —\n",
    "    merged_sas_code:       str = \"\"\n",
    "    merged_python_code:    str = \"\"\n",
    "    merged_pyspark_code:   str = \"\"\n",
    "    python_blocks_count:   int = 0\n",
    "    pyspark_blocks_count:  int = 0\n",
    "    sas_blocks_count:      int = 0\n",
    "    total_chunks_processed:int = 0\n",
    "\n",
    "    # — Validation results —\n",
    "    python_validation:   Dict[str,Any] = field(default_factory=dict)\n",
    "    pyspark_validation:  Dict[str,Any] = field(default_factory=dict)\n",
    "\n",
    "    # — Error capture —\n",
    "    error_step:         Optional[str]   = None\n",
    "    error_detail:       Optional[Any]   = None\n",
    "\n",
    "\n",
    "# --- Tool logic implementations as direct functions ---\n",
    "def remove_comments(sas_code: str) -> str:\n",
    "    sas_code = re.sub(r\"/\\*.*?\\*/\", \"\", sas_code, flags=re.DOTALL)\n",
    "    sas_code = re.sub(r\"^\\s*\\*.*?;\", \"\", sas_code, flags=re.MULTILINE)\n",
    "    return sas_code\n",
    "\n",
    "def define_sas_parser():\n",
    "    macro_start = pp.CaselessKeyword(\"%MACRO\") + pp.Word(pp.alphas + \"_\") + pp.restOfLine\n",
    "    macro_end = pp.CaselessKeyword(\"%MEND\") + pp.Optional(pp.Word(pp.alphas + \"_\")) + \";\"\n",
    "    proc_start = pp.CaselessKeyword(\"PROC\") + pp.Word(pp.alphas) + pp.restOfLine\n",
    "    proc_end = pp.CaselessKeyword(\"RUN\") + \";\"\n",
    "    proc_sql_end = pp.CaselessKeyword(\"QUIT\") + \";\"\n",
    "    data_start = pp.CaselessKeyword(\"DATA\") + pp.Word(pp.alphas + \"_\") + pp.restOfLine\n",
    "\n",
    "    macro_body = pp.originalTextFor(pp.SkipTo(macro_end, include=True))\n",
    "    data_body = pp.originalTextFor(pp.SkipTo(proc_end | proc_sql_end | macro_end, include=True)) + proc_end\n",
    "    proc_body = pp.originalTextFor(pp.SkipTo(proc_end | proc_sql_end | macro_end, include=True)) + (proc_end | proc_sql_end)\n",
    "\n",
    "    macro = macro_start + macro_body + macro_end\n",
    "    data_step = data_start + data_body\n",
    "    proc_step = proc_start + proc_body\n",
    "    return macro | data_step | proc_step\n",
    "\n",
    "def parse_sas_code(sas_code: str) -> list:\n",
    "    sas_code = remove_comments(sas_code)\n",
    "    parser = define_sas_parser()\n",
    "    parsed_blocks = parser.searchString(sas_code)\n",
    "    return [match[0] for match in parsed_blocks if match] or [sas_code]\n",
    "\n",
    "def chunk_large_blocks(chunks: list, max_chunk_size: int) -> list:\n",
    "    sub_chunks = []\n",
    "    for chunk in chunks:\n",
    "        lines = chunk.split(\"\\n\")\n",
    "        temp_chunk = []\n",
    "        for line in lines:\n",
    "            temp_chunk.append(line)\n",
    "            if len(temp_chunk) >= max_chunk_size and line.strip().upper().endswith((\"RUN;\", \"QUIT;\", \"%MEND;\")):\n",
    "                sub_chunks.append(\"\\n\".join(temp_chunk))\n",
    "                temp_chunk = []\n",
    "        if temp_chunk:\n",
    "            sub_chunks.append(\"\\n\".join(temp_chunk))\n",
    "    return sub_chunks\n",
    "\n",
    "def build_dependency_graph(chunks: list) -> nx.DiGraph:\n",
    "    dag = nx.DiGraph()\n",
    "    macro_references = {}\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        dag.add_node(i, code=chunk)\n",
    "        if \"%MACRO\" in chunk and \"%MEND\" in chunk:\n",
    "            macro_name = re.search(r'%MACRO\\s+(\\w+)', chunk, re.IGNORECASE)\n",
    "            if macro_name:\n",
    "                macro_references[macro_name.group(1)] = i\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        for macro_name, macro_index in macro_references.items():\n",
    "            if f\"%{macro_name}\" in chunk and i != macro_index:\n",
    "                dag.add_edge(macro_index, i)\n",
    "    return dag\n",
    "\n",
    "def split_overflow_chunks(chunk_list: list, max_lines: int = 400) -> list:\n",
    "    result = []\n",
    "    logical_keywords = (\"RUN;\", \"QUIT;\", \"%MEND;\")\n",
    "\n",
    "    for chunk in chunk_list:\n",
    "        lines = chunk[\"code\"].splitlines()\n",
    "        if len(lines) <= max_lines:\n",
    "            result.append(chunk)\n",
    "            continue\n",
    "\n",
    "        subchunks = []\n",
    "        start = 0\n",
    "        while start < len(lines):\n",
    "            end = min(start + max_lines, len(lines))\n",
    "            logical_end = -1\n",
    "            for i in range(end - 1, start - 1, -1):\n",
    "                if lines[i].strip().upper().endswith(logical_keywords):\n",
    "                    logical_end = i + 1\n",
    "                    break\n",
    "            if logical_end == -1 or logical_end <= start:\n",
    "                logical_end = end\n",
    "            subchunk_lines = lines[start:logical_end]\n",
    "            subchunks.append(\"\\n\".join(subchunk_lines))\n",
    "            start = logical_end\n",
    "\n",
    "        for j, sub in enumerate(subchunks):\n",
    "            result.append({\n",
    "                \"id\": f\"{chunk['id']}_sub{j+1}\",\n",
    "                \"code\": sub.strip()\n",
    "            })\n",
    "\n",
    "    return result\n",
    "\n",
    "def analyze_sas_elements(sas_code: str) -> dict:\n",
    "    return {\n",
    "        'comment': len(re.findall(r\"/\\*.*?\\*/\", sas_code, flags=re.DOTALL)) +\n",
    "                   len(re.findall(r\"^\\s*\\*.*?;\", sas_code, flags=re.MULTILINE)),\n",
    "        'macroVariableDefinition': len(re.findall(r\"%let\\s+\\w+\\s*=\", sas_code, flags=re.IGNORECASE)),\n",
    "        'libname': len(re.findall(r\"\\\\blibname\\\\b\", sas_code, flags=re.IGNORECASE)),\n",
    "        'macro': len(re.findall(r\"%macro\\\\b\", sas_code, flags=re.IGNORECASE)),\n",
    "        'macroCall': len(re.findall(r\"(?<!%)%\\\\w+\\\\b\", sas_code)),\n",
    "        'unparsedSQLStatement': len(re.findall(r\"proc\\s+sql\\\\b\", sas_code, flags=re.IGNORECASE)),\n",
    "        'procedure': len(re.findall(r\"\\\\bproc\\\\b\", sas_code, flags=re.IGNORECASE)),\n",
    "        'include': len(re.findall(r\"%include\", sas_code, flags=re.IGNORECASE)),\n",
    "        'dataStep': len(re.findall(r\"\\\\bdata\\\\b\", sas_code, flags=re.IGNORECASE)),\n",
    "    }\n",
    "\n",
    "\n",
    "# --- Nodes ---\n",
    "\n",
    "\n",
    "def chunk_sas_script_node(state: SASPipelineState) -> SASPipelineState:\n",
    "    print(\"[Node] chunk_sas_script\")\n",
    "    try:\n",
    "        script = state.script_content or \"\"\n",
    "        element_summary = analyze_sas_elements(script)\n",
    "        initial_chunks = parse_sas_code(script)\n",
    "        sub_chunks = chunk_large_blocks(initial_chunks, state.chunk_size)\n",
    "        dag = build_dependency_graph(sub_chunks)\n",
    "        try:\n",
    "            ordered_chunks = [dag.nodes[i][\"code\"] for i in nx.topological_sort(dag)]\n",
    "        except nx.NetworkXUnfeasible:\n",
    "            print(\"⚠️ Cycle detected in macro calls! Falling back to original block order.\")\n",
    "            ordered_chunks = sub_chunks\n",
    "\n",
    "        initial_result = [{\"id\": f\"blk_{i+1:03}\", \"code\": chunk.strip()} for i, chunk in enumerate(ordered_chunks)]\n",
    "        final_chunks = split_overflow_chunks(initial_result, max_lines=400)\n",
    "\n",
    "        state.chunks = [chunk[\"code\"] for chunk in final_chunks]\n",
    "        state.element_summary = element_summary\n",
    "        state.num_chunks = len(final_chunks)\n",
    "        state.script_length = len(script)\n",
    "    except Exception as e:\n",
    "        state.error_step = \"chunk_sas_script\"\n",
    "        state.error_detail = str(e)\n",
    "    return state\n",
    "\n",
    "def extract_ast_node(state: SASPipelineState) -> SASPipelineState:\n",
    "    print(\"[Node] extract_ast_node\")\n",
    "    print(f\"Extracting AST from {len(state.chunks)} chunks...\")\n",
    "\n",
    "    chunks_with_ast = []\n",
    "\n",
    "    ast_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \n",
    "     \"\"\"You are an expert SAS code analyst. Analyze SAS code chunks and return a JSON object with the following fields:\n",
    "     - type: One of [macro, dataStep, procStep, unknown]\n",
    "     - name: The name of the macro, data step, or procedure (if applicable)\n",
    "     - procType: If type is procStep, specify the PROC used (e.g., SQL, PRINT)\n",
    "     - dependencies: A list of macro calls found inside the chunk (e.g., [clean, summary])\n",
    "     - code: The raw SAS code (return exactly as received)\n",
    "     - AST: A detailed abstract syntax tree representation\n",
    "     Respond ONLY with a valid JSON object.\n",
    "    \"\"\"),\n",
    "    (\"human\", \"\"\"\n",
    "    Here is an example input and output:\n",
    "\n",
    "                SAS Code Example:\n",
    "    /*============================================================================*/\n",
    "    /*      Example SAS ETL Script - Covers All Major Constructs                  */\n",
    "    /*============================================================================*/\n",
    "\n",
    "    %let input_ds = raw.transactions;\n",
    "    %let threshold = 1000;\n",
    "    %let outlib = work;\n",
    "\n",
    "    /* Define libname for input data */\n",
    "    libname raw '/data/incoming/';\n",
    "    libname archive '/data/archive/';\n",
    "\n",
    "    /* Include a utility macro from external file */\n",
    "    %include '/sas/macros/utils.sas';\n",
    "\n",
    "    /* Define a macro to process transactions */\n",
    "    %macro process_data(in=, out=, amount_threshold=);\n",
    "\n",
    "        data &out.;\n",
    "            set &in.;\n",
    "            if amount > &amount_threshold then flag = 1;\n",
    "            else flag = 0;\n",
    "\n",
    "            /* Retain important variables across rows */\n",
    "            retain user_id last_txn_date;\n",
    "\n",
    "            /* Format date column */\n",
    "            format txn_date date9.;\n",
    "\n",
    "            /* Add a new derived column */\n",
    "            txn_year = year(txn_date);\n",
    "\n",
    "        run;\n",
    "\n",
    "        proc sort data=&out.;\n",
    "            by user_id txn_date;\n",
    "        run;\n",
    "\n",
    "        proc print data=&out.(obs=10);\n",
    "        run;\n",
    "\n",
    "    %mend process_data;\n",
    "\n",
    "    /* Call the macro with specific parameters */\n",
    "    %process_data(in=&input_ds., out=&outlib..processed_txn, amount_threshold=&threshold.);\n",
    "\n",
    "    /* Merge with reference dataset */\n",
    "    data &outlib..final_txn;\n",
    "        merge &outlib..processed_txn(in=a)\n",
    "            archive.customer_info(in=b);\n",
    "        by user_id;\n",
    "        if a;\n",
    "    run;\n",
    "\n",
    "    /* Generate basic summary with PROC SQL */\n",
    "    proc sql;\n",
    "        create table &outlib..summary as\n",
    "        select user_id, count(*) as txn_count, sum(amount) as total_amount\n",
    "        from &outlib..final_txn\n",
    "        group by user_id\n",
    "        having total_amount > &threshold;\n",
    "    quit;\n",
    "\n",
    "    /* Store results permanently if needed */\n",
    "    libname results '/data/output/';\n",
    "\n",
    "    data results.final_txn_summary;\n",
    "        set &outlib..summary;\n",
    "    run;\n",
    "\n",
    "                Expected AST Output:\n",
    "                SASProgram(\n",
    "        body=[\n",
    "            CommentBlock(\n",
    "                text='Example SAS ETL Script - Covers All Major Constructs'\n",
    "            ),\n",
    "            MacroVariableDefinition(name='input_ds', value='raw.transactions'),\n",
    "            MacroVariableDefinition(name='threshold', value='1000'),\n",
    "            MacroVariableDefinition(name='outlib', value='work'),\n",
    "\n",
    "            LibnameAssignment(alias='raw', path='/data/incoming/'),\n",
    "            LibnameAssignment(alias='archive', path='/data/archive/'),\n",
    "\n",
    "            IncludeMacro(file='/sas/macros/utils.sas'),\n",
    "\n",
    "            MacroDefinition(\n",
    "                name='process_data',\n",
    "                parameters=['in', 'out', 'amount_threshold'],\n",
    "                body=[\n",
    "                    DataStep(\n",
    "                        name='&out.',\n",
    "                        source='&in.',\n",
    "                        operations=[\n",
    "                            ConditionalAssignment(\n",
    "                                condition='amount > &amount_threshold',\n",
    "                                then='flag = 1',\n",
    "                                else_='flag = 0'\n",
    "                            ),\n",
    "                            RetainStatement(variables=['user_id', 'last_txn_date']),\n",
    "                            FormatStatement(variable='txn_date', format='date9.'),\n",
    "                            DerivedColumnAssignment(\n",
    "                                name='txn_year',\n",
    "                                expression='year(txn_date)'\n",
    "                            )\n",
    "                        ]\n",
    "                    ),\n",
    "                    ProcSort(\n",
    "                        data='&out.',\n",
    "                        by=['user_id', 'txn_date']\n",
    "                    ),\n",
    "                    ProcPrint(\n",
    "                        data='&out.',\n",
    "                        options={{'obs': 10}}\n",
    "                    )\n",
    "                ]\n",
    "            ),\n",
    "\n",
    "            MacroCall(\n",
    "                name='process_data',\n",
    "                arguments={{\n",
    "                    'in': '&input_ds.',\n",
    "                    'out': '&outlib..processed_txn',\n",
    "                    'amount_threshold': '&threshold.'\n",
    "                }}\n",
    "            ),\n",
    "\n",
    "            DataStep(\n",
    "                name='&outlib..final_txn',\n",
    "                source='MERGE',\n",
    "                merge_sources=[\n",
    "                    {{'dataset': '&outlib..processed_txn', 'alias': 'a'}},\n",
    "                    {{'dataset': 'archive.customer_info', 'alias': 'b'}}\n",
    "                ],\n",
    "                by='user_id',\n",
    "                filter='if a'\n",
    "            ),\n",
    "\n",
    "            ProcSQL(\n",
    "                operation='create table',\n",
    "                table='&outlib..summary',\n",
    "                select=[\n",
    "                    'user_id',\n",
    "                    'count(*) as txn_count',\n",
    "                    'sum(amount) as total_amount'\n",
    "                ],\n",
    "                from_='&outlib..final_txn',\n",
    "                group_by='user_id',\n",
    "                having='total_amount > &threshold'\n",
    "            ),\n",
    "\n",
    "            LibnameAssignment(alias='results', path='/data/output/'),\n",
    "\n",
    "            DataStep(\n",
    "                name='results.final_txn_summary',\n",
    "                source='&outlib..summary'\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "                Now analyze the following SAS code chunk:\n",
    "                {chunk_code}\n",
    "\n",
    "                Expected JSON output:\n",
    "                \"\"\")\n",
    "])\n",
    "    ast_chain = ast_prompt | llm | JsonOutputParser()\n",
    "\n",
    "    for i, chunk in enumerate(state.chunks):\n",
    "        try:\n",
    "            print(f\"Processing chunk {i+1}/{len(state.chunks)}\")\n",
    "            ast_result = ast_chain.invoke({\"chunk_code\": chunk})\n",
    "            chunk_ast = ast_result.get(\"AST\", \"No AST generated\")\n",
    "\n",
    "            chunks_with_ast.append({\n",
    "                \"chunk_id\": f\"chunk_{i+1:03}\",\n",
    "                \"chunk_code\": chunk,\n",
    "                \"chunk_ast\": chunk_ast\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk {i+1}: {e}\")\n",
    "            chunks_with_ast.append({\n",
    "                \"chunk_id\": f\"chunk_{i+1:03}\",\n",
    "                \"chunk_code\": chunk,\n",
    "                \"chunk_ast\": f\"Error generating AST: {str(e)}\"\n",
    "            })\n",
    "\n",
    "    state.chunks_with_ast = chunks_with_ast\n",
    "    return state\n",
    "\n",
    "def translate_chunk_node(state: SASPipelineState) -> SASPipelineState:\n",
    "    \"\"\"\n",
    "    Translate SAS chunks to Python and PySpark using AST-only iterative translation\n",
    "    \"\"\"\n",
    "    print(\"[Node] translate_chunk_node\")\n",
    "    print(f\"Translating {len(state.chunks_with_ast)} SAS chunks to Python and PySpark...\")\n",
    "\n",
    "    translated_chunks = []\n",
    "    accumulated_python_code = \"\"\n",
    "    accumulated_pyspark_code = \"\"\n",
    "\n",
    "    translation_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"\n",
    "            You are a SAS-to-Python/PySpark translator. Convert AST to equivalent code.\n",
    "\n",
    "          CRITICAL RULES:\n",
    "          1. Use EXACT names from AST: If AST shows 'name': 'XYZ', function = 'XYZ' \n",
    "          2. Use EXACT table names: 'BDCC.Searches' → 'bdcc_searches_df' parameter\n",
    "          3. Implement ALL AST operations precisely - no simplification\n",
    "          4. Reuse existing imports from previous code - never duplicate\n",
    "          5. Connect chunks: output of previous = input of current\n",
    "\n",
    "          AST MAPPING:\n",
    "          - MacroDefinition.name → function name\n",
    "          - Table names → DataFrame parameters  \n",
    "          - ConditionalOutput → proper if/else logic\n",
    "          - RetainStatement → window functions/groupby logic\n",
    "          - DeleteStatement → filter operations\n",
    "\n",
    "          Return JSON, without back tick ```json :\n",
    "          {{\"python_code\": \"pandas code\", \"pyspark_code\": \"spark code\", \"translation_notes\": \"connections made\"}}\n",
    "          \"\"\"),\n",
    "        (\"human\", \"\"\"\n",
    "          AST: {chunk_ast}\n",
    "          Previous Python: {previous_python_code}\n",
    "          Previous PySpark: {previous_pyspark_code}\n",
    "\n",
    "          Requirements:\n",
    "          1. Function name = AST.name exactly\n",
    "          2. Implement ALL AST body operations\n",
    "          3. Reuse imports/variables from previous code\n",
    "          4. Connect data flow logically\n",
    "            \"\"\")\n",
    "    ])\n",
    "    \n",
    "    translation_chain = translation_prompt | llm | JsonOutputParser()\n",
    "\n",
    "    for i, chunk_data in enumerate(state.chunks_with_ast):\n",
    "        print(f\"Translating chunk {i+1}/{len(state.chunks_with_ast)}\")\n",
    "\n",
    "        try:\n",
    "            chunk_id = chunk_data.get(\"chunk_id\", f\"chunk_{i+1}\")\n",
    "            chunk_ast = chunk_data.get(\"chunk_ast\", \"\")\n",
    "\n",
    "            # Skip if AST is empty or error\n",
    "            if not chunk_ast or (isinstance(chunk_ast, str) and chunk_ast.startswith(\"Error\")):\n",
    "                print(f\"Skipping chunk {i+1} due to invalid AST\")\n",
    "                translated_chunks.append({\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"chunk_ast\": chunk_ast,\n",
    "                    \"chunk_python_code\": \"# Skipped: Invalid AST\",\n",
    "                    \"chunk_pyspark_code\": \"# Skipped: Invalid AST\",\n",
    "                    \"translation_notes\": \"Skipped due to AST extraction error\"\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            # Convert AST to string representation for the prompt\n",
    "            if isinstance(chunk_ast, dict):\n",
    "                ast_str = json.dumps(chunk_ast, indent=2)\n",
    "            else:\n",
    "                ast_str = str(chunk_ast)\n",
    "\n",
    "            result = translation_chain.invoke({\n",
    "                \"chunk_ast\": ast_str,\n",
    "                \"previous_python_code\": accumulated_python_code,\n",
    "                \"previous_pyspark_code\": accumulated_pyspark_code\n",
    "            })\n",
    "\n",
    "            python_code = result.get(\"python_code\", \"\")\n",
    "            pyspark_code = result.get(\"pyspark_code\", \"\")\n",
    "            notes = result.get(\"translation_notes\", \"\")\n",
    "\n",
    "            translated_chunks.append({\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"chunk_ast\": chunk_ast,\n",
    "                \"chunk_python_code\": python_code,\n",
    "                \"chunk_pyspark_code\": pyspark_code,\n",
    "                \"translation_notes\": notes\n",
    "            })\n",
    "\n",
    "            # Accumulate code for context in next iteration\n",
    "            if python_code and not python_code.startswith(\"#\"):\n",
    "                accumulated_python_code += f\"\\n\\n# --- Chunk {i+1} ---\\n{python_code}\"\n",
    "            if pyspark_code and not pyspark_code.startswith(\"#\"):\n",
    "                accumulated_pyspark_code += f\"\\n\\n# --- Chunk {i+1} ---\\n{pyspark_code}\"\n",
    "\n",
    "            # Trim accumulated code if it gets too long (keep last 3000 chars for context)\n",
    "            if len(accumulated_python_code) > 3000:\n",
    "                accumulated_python_code = \"...\\n\" + accumulated_python_code[-3000:]\n",
    "            if len(accumulated_pyspark_code) > 3000:\n",
    "                accumulated_pyspark_code = \"...\\n\" + accumulated_pyspark_code[-3000:]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error translating chunk {i+1}: {str(e)}\")\n",
    "            translated_chunks.append({\n",
    "                \"chunk_id\": chunk_data.get(\"chunk_id\", f\"chunk_{i+1}\"),\n",
    "                \"chunk_ast\": chunk_data.get(\"chunk_ast\", \"\"),\n",
    "                \"chunk_python_code\": f\"# Translation Error: {str(e)}\",\n",
    "                \"chunk_pyspark_code\": f\"# Translation Error: {str(e)}\",\n",
    "                \"translation_notes\": f\"Translation failed: {str(e)}\"\n",
    "            })\n",
    "\n",
    "    state.translated_chunks = translated_chunks\n",
    "    print(f\"✅ Translation complete: {len(translated_chunks)} chunks processed\")\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "def merge_code_node(state: SASPipelineState) -> SASPipelineState:\n",
    "    print(\"[Node] merge_code_node\")\n",
    "    print(\"Merging and deduplicating code blocks...\")\n",
    "\n",
    "    try:\n",
    "        python_blocks = []\n",
    "        pyspark_blocks = []\n",
    "        sas_blocks = []\n",
    "\n",
    "        for chunk in state.translated_chunks:\n",
    "            python_code = chunk.get(\"chunk_python_code\", \"\")\n",
    "            pyspark_code = chunk.get(\"chunk_pyspark_code\", \"\")\n",
    "            sas_code = chunk.get(\"chunk_sas_code\", \"\")\n",
    "\n",
    "            if python_code.strip():\n",
    "                python_blocks.append(python_code)\n",
    "            if pyspark_code.strip():\n",
    "                pyspark_blocks.append(pyspark_code)\n",
    "            if sas_code.strip():\n",
    "                sas_blocks.append(sas_code)\n",
    "\n",
    "        unique_python_blocks = list(set(python_blocks))\n",
    "        unique_pyspark_blocks = list(set(pyspark_blocks))\n",
    "        unique_sas_blocks = list(set(sas_blocks))\n",
    "\n",
    "        now = datetime.now()\n",
    "\n",
    "        state.merged_python_code = (\n",
    "            \"# Python Code Generated from SAS Translation\\n\"\n",
    "            f\"# Generated on: {now}\\n\\n\" +\n",
    "            \"\\n\\n\".join(unique_python_blocks)\n",
    "        )\n",
    "\n",
    "        state.merged_pyspark_code = (\n",
    "            \"# PySpark Code Generated from SAS Translation\\n\"\n",
    "            f\"# Generated on: {now}\\n\\n\" +\n",
    "            \"\\n\\n\".join(unique_pyspark_blocks)\n",
    "        )\n",
    "\n",
    "        state.merged_sas_code = (\n",
    "            \"# Original SAS Code\\n\"\n",
    "            f\"# Generated on: {now}\\n\\n\" +\n",
    "            \"\\n\\n\".join(unique_sas_blocks)\n",
    "        )\n",
    "\n",
    "        state.python_blocks_count = len(unique_python_blocks)\n",
    "        state.pyspark_blocks_count = len(unique_pyspark_blocks)\n",
    "        state.sas_blocks_count = len(unique_sas_blocks)\n",
    "        state.total_chunks_processed = len(state.translated_chunks)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during code merging: {str(e)}\")\n",
    "        state.merged_sas_code = \"# Error in code merging\"\n",
    "        state.merged_python_code = \"# Error in code merging\"\n",
    "        state.merged_pyspark_code = \"# Error in code merging\"\n",
    "        state.error_step = \"merge\"\n",
    "        state.error_detail = str(e)\n",
    "\n",
    "    return state\n",
    "\n",
    "def validate_python_node(state: SASPipelineState) -> SASPipelineState:\n",
    "    \n",
    "    print(\"[NODE] validate_python_code\\nValidating Python syntax...\")\n",
    "\n",
    "    code = state.merged_python_code\n",
    "    try:\n",
    "        # Parse code\n",
    "        try:\n",
    "            ast.parse(code)\n",
    "            syntax_valid = True\n",
    "            syntax_errors = []\n",
    "        except SyntaxError as e:\n",
    "            syntax_valid = False\n",
    "            syntax_errors = [{\n",
    "                \"line\": e.lineno,\n",
    "                \"column\": e.offset,\n",
    "                \"message\": e.msg\n",
    "            }]\n",
    "        except Exception as e:\n",
    "            syntax_valid = False\n",
    "            syntax_errors = [{\"message\": str(e)}]\n",
    "\n",
    "        # Warnings\n",
    "        warnings = []\n",
    "        imported_modules = re.findall(r'import\\s+(\\w+)', code)\n",
    "        if 'pandas' in code and not any(mod in ['pandas', 'pd'] for mod in imported_modules):\n",
    "            warnings.append(\"pandas usage detected but not imported\")\n",
    "        if 'numpy' in code and not any(mod in ['numpy', 'np'] for mod in imported_modules):\n",
    "            warnings.append(\"numpy usage detected but not imported\")\n",
    "        if 'spark' in code and 'pyspark' not in code:\n",
    "            warnings.append(\"PySpark usage detected but pyspark not imported\")\n",
    "\n",
    "        # Update state\n",
    "        state.python_validation = {\n",
    "            \"syntax_valid\": syntax_valid,\n",
    "            \"syntax_errors\": syntax_errors,\n",
    "            \"warnings\": warnings,\n",
    "            \"code_length\": len(code),\n",
    "            \"line_count\": len(code.splitlines())\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        state.python_validation = {\n",
    "            \"syntax_valid\": False,\n",
    "            \"error\": str(e),\n",
    "            \"syntax_errors\": [{\"message\": f\"Validation error: {str(e)}\"}]\n",
    "        }\n",
    "        state.error_step = \"python_validation\"\n",
    "        state.error_detail = str(e)\n",
    "\n",
    "    return state\n",
    "\n",
    "def validate_pyspark_node(state: SASPipelineState) -> SASPipelineState:\n",
    "    print(\"[NODE] Validate pyspark code \\n Validating PySpark syntax...\")\n",
    "\n",
    "    code = state.merged_pyspark_code\n",
    "    try:\n",
    "        try:\n",
    "            ast.parse(code)\n",
    "            basic_syntax_valid = True\n",
    "            syntax_errors = []\n",
    "        except SyntaxError as e:\n",
    "            basic_syntax_valid = False\n",
    "            syntax_errors = [{\n",
    "                \"line\": e.lineno,\n",
    "                \"column\": e.offset,\n",
    "                \"message\": e.msg\n",
    "            }]\n",
    "        except Exception as e:\n",
    "            basic_syntax_valid = False\n",
    "            syntax_errors = [{\"message\": str(e)}]\n",
    "\n",
    "        # Warnings and pattern detection\n",
    "        warnings = []\n",
    "        detected_patterns = []\n",
    "        pyspark_patterns = [\n",
    "            (r'spark\\.', \"spark object usage\"),\n",
    "            (r'\\.read\\.', \"DataFrameReader usage\"),\n",
    "            (r'\\.write\\.', \"DataFrameWriter usage\"),\n",
    "            (r'\\.select\\(', \"select operation\"),\n",
    "            (r'\\.filter\\(', \"filter operation\"),\n",
    "            (r'\\.groupBy\\(', \"groupBy operation\"),\n",
    "            (r'\\.agg\\(', \"aggregation operation\"),\n",
    "            (r'\\.join\\(', \"join operation\")\n",
    "        ]\n",
    "\n",
    "        for pattern, description in pyspark_patterns:\n",
    "            if re.search(pattern, code):\n",
    "                detected_patterns.append(description)\n",
    "\n",
    "        if any(kw in code for kw in ['spark.', 'SparkSession', 'DataFrame']):\n",
    "            if 'pyspark' not in code and 'SparkSession' not in code:\n",
    "                warnings.append(\"PySpark usage detected but SparkSession not properly initialized\")\n",
    "\n",
    "        # Update state\n",
    "        state.pyspark_validation = {\n",
    "            \"basic_syntax_valid\": basic_syntax_valid,\n",
    "            \"syntax_errors\": syntax_errors,\n",
    "            \"warnings\": warnings,\n",
    "            \"detected_pyspark_patterns\": detected_patterns,\n",
    "            \"code_length\": len(code),\n",
    "            \"line_count\": len(code.splitlines())\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        state.pyspark_validation = {\n",
    "            \"basic_syntax_valid\": False,\n",
    "            \"error\": str(e),\n",
    "            \"syntax_errors\": [{\"message\": f\"Validation error: {str(e)}\"}]\n",
    "        }\n",
    "        state.error_step = \"pyspark_validation\"\n",
    "        state.error_detail = str(e)\n",
    "\n",
    "    return state\n",
    "\n",
    "# --- Graph ---\n",
    "graph = StateGraph(SASPipelineState)\n",
    "graph.add_node(\"chunk_node\", chunk_sas_script_node)\n",
    "graph.add_node(\"ast_node\", extract_ast_node)\n",
    "graph.add_node(\"translate_node\", translate_chunk_node)\n",
    "graph.add_node(\"merge_node\", merge_code_node)\n",
    "graph.add_node(\"validate_python\", validate_python_node)\n",
    "graph.add_node(\"validate_pyspark\", validate_pyspark_node)\n",
    "\n",
    "graph.add_edge(START, \"chunk_node\")\n",
    "graph.add_edge(\"chunk_node\", \"ast_node\")\n",
    "graph.add_edge(\"ast_node\", \"translate_node\")\n",
    "graph.add_edge(\"translate_node\", \"merge_node\")\n",
    "graph.add_edge(\"merge_node\", \"validate_python\")\n",
    "graph.add_edge(\"validate_python\", \"validate_pyspark\")\n",
    "graph.add_edge(\"validate_pyspark\", END)\n",
    "\n",
    "pipeline = graph.compile()\n",
    "\n",
    "#---Test the sample script----\n",
    "\n",
    "# query= \"\"\"Analysis script\"\"\"\n",
    "# # from IPython.display import Image\n",
    "# # Image(pipeline.get_graph().draw_mermaid_png())\n",
    "\n",
    "\n",
    "# init_state = SASPipelineState(\n",
    "#     file_path=None,\n",
    "#     script_content=query,\n",
    "#     chunk_size=100\n",
    "# )\n",
    "# raw = pipeline.invoke(init_state,stream_mode=\"debug\",config={\"configurable\": {\"thread_id\": \"1\"}})\n",
    "# final = SASPipelineState(**raw)\n",
    "\n",
    "# # Inspect final state\n",
    "# print(\"Chunks:\",final.chunks)\n",
    "# print(f\"DEBUG ==>> LEN : {len(final.chunks)}\")\n",
    "# print(\"AST list length:\",         len(final.chunks_with_ast))\n",
    "# print(\"Translated chunks:\",       len(final.translated_chunks))\n",
    "# print(\"Merged Python code length:\", len(final.merged_python_code))\n",
    "# print(\"Python syntax valid:\",     final.python_validation.get(\"syntax_valid\"))\n",
    "# print(\"PySpark syntax valid:\",    final.pyspark_validation.get(\"basic_syntax_valid\"))\n",
    "# if final.error_step:\n",
    "#     print(\"Error at step:\", final.error_step)\n",
    "#     print(\"Error detail:\", final.error_detail)\n",
    "\n",
    "\n",
    "# ##Testing the graph streaming here:: \n",
    "\n",
    "# for chunk in pipeline.stream(init_state, stream_mode=\"values\",config={\"configurable\": {\"thread_id\": \"1\"}}):\n",
    "#     print(f\"Type of the chunk {i+1} : {type(chunk)}\")\n",
    "    \n",
    "#     print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08ec703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---Test the sample script----\n",
    "\n",
    "query= r\"\"\"%macro MBSearches();\n",
    "\n",
    "\n",
    "\t/**** CALL CREDIT ****/\n",
    "\tPROC SQL;\n",
    "\t   CREATE TABLE WORK.QUERY_FOR_SEARCHES AS \n",
    "\t   SELECT t1.PersonID, \n",
    "\t          t1.SearchOrgTypeID, \n",
    "\t          t1.SearchHistoryDate, \n",
    "\t          t1.SearchReference, \n",
    "\t          t1.SearchPurposeID\n",
    "\t      FROM BDCC.Searches t1;\n",
    "\tQUIT;\n",
    "\n",
    "\tdata work.CC_SEARCH_NORM work.CC_NONCREDSEARCH_NORM;\n",
    "\tset WORK.query_for_searches;\n",
    "\n",
    "\t/*Format applicationdate*/\n",
    "\tformat applicationdate date9.;\n",
    "\tapplicationdate = datepart(searchhistorydate);\n",
    "\tpersonID_CC = personID;\n",
    "\t/*Set Bureau to C*/\n",
    "\tbureau=\"C\";\n",
    "\t/*CA is credit application*/\n",
    "\tif SearchPurposeID = 12 then output work.CC_SEARCH_NORM;\n",
    "\telse\n",
    "\toutput work.CC_NONCREDSEARCH_NORM;\n",
    "\n",
    "\tkeep personID_CC applicationdate bureau;\n",
    "\trun;\n",
    "\n",
    "\n",
    "\t/**** EXPERIAN ****/\n",
    "\tPROC SQL;\n",
    "\t   CREATE TABLE WORK.EXPERIAN_SEARCHES AS \n",
    "\t   SELECT t1.PersonID, \n",
    "\t          t2.ApplicationDate, \n",
    "\t          t2.ApplicationTypeID\n",
    "\t      FROM BDATA.ExCreditApplicationMapping t1\n",
    "\t           INNER JOIN BDATA.ExCreditApplication t2 ON (t1.ExCreditapplicationID = t2.ExCreditApplicationID);\n",
    "\tQUIT;\n",
    "\n",
    "\tdata work.EXP_Search_Norm;\n",
    "\tset work.experian_searches;\n",
    "\tbureau=\"E\";\n",
    "\trun;\n",
    "\n",
    "\n",
    "\t/**** JOIN DATA ****/\n",
    "\tPROC SQL;\n",
    "\t\tcreate table work.EXP_SEARCH_WITHID as select distinct\n",
    "\t\t\tb.NEWID\n",
    "\t\t\t,a.applicationdate\n",
    "\t\t\t,a.bureau\n",
    "\t\tfrom work.EXP_SEARCH_NORM as a\n",
    "\t\tinner join work.completedsample as b on (a.PersonID = b.PersonID);\n",
    "\tquit;\n",
    "\n",
    "\tproc sql;\n",
    "\t\tcreate table work.CC_Search_WithID as select distinct\n",
    "\t\t\tb.NEWID\n",
    "\t\t\t,a.applicationdate\n",
    "\t\t\t,a.bureau\n",
    "\t\tfrom work.CC_SEARCH_NORM as a\n",
    "\t\tinner join work.completedsample as b on (a.personid_cc = b.personid_cc);\n",
    "\tquit;\n",
    "\n",
    "\tdata work.ALL_SEARCH_APPEND;\n",
    "\tset work.CC_Search_WithID work.Exp_Search_WithID;\n",
    "\trun;\n",
    "\n",
    "\tproc sql;\n",
    "\t\tdrop table work.exp_search_withid,\n",
    "\t\twork.cc_search_withid,\n",
    "\t\twork.QUERY_FOR_SEARCHES,\n",
    "\t\twork.EXPERIAN_SEARCHES,\n",
    "\t\twork.CC_SEARCH_NORM,\n",
    "\t\twork.EXP_SEARCH_NORM;\n",
    "\tquit;\n",
    "\n",
    "\n",
    "\t/**** DE-DUP ****/\n",
    "\n",
    "\tproc sort data=work.all_search_append out=work.sort_search_append;\n",
    "\tby NewID applicationdate;\n",
    "\trun;\n",
    "\n",
    "\tdata work.MB_SearchData (drop=temp_applicationdate temp_bureau);\n",
    "\tset work.sort_search_append;\n",
    "\tby NewID applicationdate;\n",
    "\tretain temp_applicationdate temp_bureau;\n",
    "\n",
    "\tif first.NewID then do;\n",
    "\t\ttemp_applicationdate = applicationdate;\n",
    "\t\ttemp_bureau = bureau;\n",
    "\tend;\n",
    "\telse do;\n",
    "\t\tif temp_applicationdate = applicationdate\n",
    "\t\t\tand\n",
    "\t\t\t\ttemp_bureau ne bureau\n",
    "\t\t\t\t\tthen delete;\n",
    "\t\telse do;\n",
    "\t\ttemp_applicationdate = applicationdate;\n",
    "\t\ttemp_bureau = bureau;\n",
    "\t\tend;\n",
    "\tend;\n",
    "\n",
    "\trun;\n",
    "\n",
    "\tproc sql;\n",
    "\tcreate table work.ecaps as\n",
    "\tselect t1.*,\n",
    "\tt2.debt_code as accountnumber,\n",
    "\tt2.sampledate\n",
    "\tFROM MB_SearchData t1\n",
    "\tINNER JOIN WORK.COMPLETEDSAMPLE_ACC t2 on (t1.NewID = t2.NewID);\n",
    "\tquit;\n",
    "\n",
    "\tproc sort data=ecaps;\n",
    "\tBY accountnumber sampledate;\n",
    "\trun;\n",
    "\n",
    "\tproc sql;\n",
    "\t\tdrop table \n",
    "\t\twork.sort_search_append,\n",
    "\t\tall_search_append,\n",
    "\t\tMB_SearchData\n",
    "\t;\n",
    "\tquit;\n",
    "\n",
    "\t/**** VARIABLES ****/\n",
    "\n",
    "\t%macro searchvars(type);\n",
    "\t\t\n",
    "\t\tNumSearches_&type=0;\n",
    "\t\tNumSearchesL12M_&type=0;\n",
    "\t\tNumSearchesL6M_&type=0;\n",
    "\t\tNumSearchesL3M_&type=0;\n",
    "\n",
    "\t\tformat MostRecentSearch_&type date9.;\n",
    "\t\tMostRecentSearch_&type='.'d;\t\n",
    "\n",
    "\t/*\tlength hurn_retain_&type $20000;\n",
    "\t\thurn_retain_&type='';\n",
    "\t\tNumAddsSearched_&type=0;*/\n",
    "\n",
    "\t%mend;\n",
    "\n",
    "\n",
    "\n",
    "\t%macro searches(type);\n",
    "\n",
    "\t\tif\tapplicationdate<sampledate then NumSearches_&type+1;\n",
    "\t\tif\tintnx('month',sampledate,-12,'s')<applicationdate<sampledate then NumSearchesL12M_&type+1;\n",
    "\t\tif\tintnx('month',sampledate,-6,'s')<applicationdate<sampledate then NumSearchesL6M_&type+1;\n",
    "\t\tif\tintnx('month',sampledate,-3,'s')<applicationdate<sampledate then NumSearchesL3M_&type+1;\n",
    "\n",
    "\t\tMostRecentSearch_&type=max(MostRecentSearch_&type,applicationdate);\n",
    "\n",
    "\t/*\tif indexw(compress(hurn_retain_&type),compress(put(locationid,20.)),'#')=0 then do;\n",
    "\t\t\tNumAddsSearched_&type+1;\n",
    "\t\t\thurn_retain_&type=catx('#',hurn_retain_&type,locationid);\n",
    "\t\tend;*/\n",
    "\n",
    "\t%mend;\n",
    "\n",
    "\n",
    "\n",
    "\t%macro searchcalcs(type);\n",
    "\n",
    "\t\tif missing(MostRecentSearch_&type)=0 then MonthsRecentSearch_&type = intck('month',MostRecentSearch_&type,sampledate);\t\t\t\t\t\t\t\telse AgeRecentSearch_&type = -999999;\n",
    "\n",
    "\t/*\tdrop hurn_retain_&type MostRecentSearch_&type;*/\n",
    "\n",
    "\t%mend;\n",
    "\n",
    "\t%let out_data=ecaps_summary;\n",
    "\n",
    "\t%put &out_data;\n",
    "\n",
    "\tdata work.&out_data (drop=bureau applicationdate mostrecentsearch_all);\n",
    "\t\tset ecaps;\n",
    "\t\tby AccountNumber SampleDate;\n",
    "\n",
    "\t\tif first.SampleDate then do;\n",
    "\t\t\t%searchvars(ALL);\n",
    "\t\tend;\n",
    "\n",
    "\n",
    "\t\tretain _all_;\n",
    "\n",
    "\t\tif applicationdate>SampleDate then delete;\n",
    "\n",
    "\t\t%searches(ALL);\n",
    "\n",
    "\n",
    "\t\tif last.SampleDate then do;\n",
    "\t\t\t%searchcalcs(All);\n",
    "\t\t\toutput;\n",
    "\t\tend;\n",
    "\n",
    "\trun;\n",
    "\n",
    "\tproc sql;\n",
    "\tdrop table work.ecaps;\n",
    "\tquit;\n",
    "\n",
    "%mend;\n",
    "\"\"\"\n",
    "# from IPython.display import Image\n",
    "# Image(pipeline.get_graph().draw_mermaid_png())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc42eb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Node] chunk_sas_script\n",
      "[Node] extract_ast_node\n",
      "Extracting AST from 2 chunks...\n",
      "Processing chunk 1/2\n",
      "Processing chunk 2/2\n",
      "[Node] translate_chunk_node\n",
      "Translating 2 SAS chunks to Python and PySpark...\n",
      "Translating chunk 1/2\n",
      "Translating chunk 2/2\n",
      "✅ Translation complete: 2 chunks processed\n",
      "[Node] merge_code_node\n",
      "Merging and deduplicating code blocks...\n",
      "[NODE] validate_python_code\n",
      "Validating Python syntax...\n",
      "[NODE] Validate pyspark code \n",
      " Validating PySpark syntax...\n"
     ]
    }
   ],
   "source": [
    "init_state = SASPipelineState(\n",
    "    file_path=None,\n",
    "    script_content=query,\n",
    "    chunk_size=100\n",
    ")\n",
    "raw = pipeline.invoke(init_state,stream_mode=\"values\",config={\"configurable\": {\"thread_id\": \"1\"}})\n",
    "final = SASPipelineState(**raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83ac0a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# PySpark Code Generated from SAS Translation\n",
      "# Generated on: 2025-07-29 02:30:12.636534\n",
      "\n",
      "from pyspark.sql import SparkSession\n",
      "from pyspark.sql.functions import col, lit, when, date_format\n",
      "\n",
      "def process_searches(searches_df, ex_credit_app_mapping_df, ex_credit_app_df, completed_sample_df):\n",
      "    # Step 1: Create QUERY_FOR_SEARCHES\n",
      "    query_for_searches = searches_df.select('PersonID', 'SearchOrgTypeID', 'SearchHistoryDate', 'SearchReference', 'SearchPurposeID')\n",
      "\n",
      "    # Step 2: Normalize CC_SEARCH_NORM and CC_NONCREDSEARCH_NORM\n",
      "    query_for_searches = query_for_searches.withColumn('applicationdate', date_format(col('SearchHistoryDate'), 'yyyy-MM-dd'))\n",
      "    query_for_searches = query_for_searches.withColumn('personID_CC', col('PersonID'))\n",
      "\n",
      "    cc_search_norm = query_for_searches.filter(col('SearchPurposeID') == 12).select('personID_CC', 'applicationdate').withColumn('bureau', lit('CC'))\n",
      "    cc_noncredsearch_norm = query_for_searches.filter(col('SearchPurposeID') != 12).select('personID_CC', 'applicationdate').withColumn('bureau', lit('NONCRED'))\n",
      "\n",
      "    # Step 3: Create EXPERIAN_SEARCHES\n",
      "    experian_searches = ex_credit_app_mapping_df.join(ex_credit_app_df, ex_credit_app_mapping_df['ExCreditapplicationID'] == ex_credit_app_df['ExCreditApplicationID'], 'inner')\n",
      "    experian_searches = experian_searches.select('PersonID', 'ApplicationDate', 'ApplicationTypeID')\n",
      "\n",
      "    # Step 4: Normalize EXP_Search_Norm\n",
      "    exp_search_norm = experian_searches.withColumn('bureau', lit('E'))\n",
      "\n",
      "    # Step 5: Create EXP_SEARCH_WITHID\n",
      "    exp_search_withid = exp_search_norm.join(completed_sample_df, 'PersonID', 'inner').select('NEWID', 'ApplicationDate', 'bureau')\n",
      "\n",
      "    # Step 6: Create CC_Search_WithID\n",
      "    cc_search_withid = cc_search_norm.join(completed_sample_df, cc_search_norm['personID_CC'] == completed_sample_df['PersonID'], 'inner').select('NEWID', 'applicationdate', 'bureau')\n",
      "\n",
      "    # Step 7: Append ALL_SEARCH_APPEND\n",
      "    all_search_append = cc_search_withid.union(exp_search_withid)\n",
      "\n",
      "    # Step 8: Sort ALL_SEARCH_APPEND\n",
      "    sort_search_append = all_search_append.orderBy(['NEWID', 'applicationdate'])\n",
      "\n",
      "    # Step 9: Process MB_SearchData\n",
      "    from pyspark.sql.window import Window\n",
      "    from pyspark.sql.functions import lag\n",
      "\n",
      "    window_spec = Window.partitionBy('NEWID').orderBy('applicationdate')\n",
      "\n",
      "    sort_search_append = sort_search_append.withColumn('temp_applicationdate', lag('applicationdate').over(window_spec))\n",
      "    sort_search_append = sort_search_append.withColumn('temp_bureau', lag('bureau').over(window_spec))\n",
      "\n",
      "    mb_search_data = sort_search_append.filter((col('temp_applicationdate') != col('applicationdate')) | (col('temp_bureau') != col('bureau')))\n",
      "\n",
      "    return mb_search_data\n",
      "\n",
      "from pyspark.sql import functions as F\n",
      "from pyspark.sql.window import Window\n",
      "\n",
      "def create_ecaps(mb_search_data, completed_sample_df):\n",
      "    # Step 1: Join MB_SearchData with COMPLETEDSAMPLE_ACC\n",
      "    ecaps = mb_search_data.join(completed_sample_df, mb_search_data['NEWID'] == completed_sample_df['NewID'], 'inner')\n",
      "    ecaps = ecaps.select('NEWID', 'applicationdate', 'bureau', F.col('debt_code').alias('accountnumber'), 'sampledate')\n",
      "\n",
      "    # Step 2: Sort ECAPS by accountnumber and sampledate\n",
      "    ecaps = ecaps.orderBy(['accountnumber', 'sampledate'])\n",
      "\n",
      "    return ecaps\n",
      "\n",
      "def process_search_vars():\n",
      "    # Initialize search variables\n",
      "    search_vars = {\n",
      "        'NumSearches_ALL': 0,\n",
      "        'NumSearchesL12M_ALL': 0,\n",
      "        'NumSearchesL6M_ALL': 0,\n",
      "        'NumSearchesL3M_ALL': 0,\n",
      "        'MostRecentSearch_ALL': None\n",
      "    }\n",
      "    return search_vars\n",
      "\n",
      "def update_searches(ecaps, search_vars):\n",
      "    # Update search variables based on conditions\n",
      "    ecaps = ecaps.withColumn('NumSearches_ALL', F.when(ecaps['applicationdate'] < ecaps['sampledate'], search_vars['NumSearches_ALL'] + 1).otherwise(search_vars['NumSearches_ALL']))\n",
      "    ecaps = ecaps.withColumn('NumSearchesL12M_ALL', F.when((F.add_months(ecaps['sampledate'], -12) < ecaps['applicationdate']) & (ecaps['applicationdate'] < ecaps['sampledate']), search_vars['NumSearchesL12M_ALL'] + 1).otherwise(search_vars['NumSearchesL12M_ALL']))\n",
      "    ecaps = ecaps.withColumn('NumSearchesL6M_ALL', F.when((F.add_months(ecaps['sampledate'], -6) < ecaps['applicationdate']) & (ecaps['applicationdate'] < ecaps['sampledate']), search_vars['NumSearchesL6M_ALL'] + 1).otherwise(search_vars['NumSearchesL6M_ALL']))\n",
      "    ecaps = ecaps.withColumn('NumSearchesL3M_ALL', F.when((F.add_months(ecaps['sampledate'], -3) < ecaps['applicationdate']) & (ecaps['applicationdate'] < ecaps['sampledate']), search_vars['NumSearchesL3M_ALL'] + 1).otherwise(search_vars['NumSearchesL3M_ALL']))\n",
      "    ecaps = ecaps.withColumn('MostRecentSearch_ALL', F.greatest(F.col('MostRecentSearch_ALL'), ecaps['applicationdate']))\n",
      "\n",
      "    return ecaps\n",
      "\n",
      "def calculate_search_metrics(ecaps):\n",
      "    # Calculate additional metrics\n",
      "    ecaps = ecaps.withColumn('MonthsRecentSearch_ALL', F.when(F.col('MostRecentSearch_ALL').isNotNull(), F.months_between(ecaps['sampledate'], ecaps['MostRecentSearch_ALL'])).otherwise(-999999))\n",
      "\n",
      "    return ecaps\n",
      "\n",
      "def summarize_ecaps(ecaps):\n",
      "    # Summarize ECAPS data\n",
      "    window_spec = Window.partitionBy('accountnumber', 'sampledate')\n",
      "\n",
      "    ecaps = process_search_vars()\n",
      "    ecaps = update_searches(ecaps, search_vars)\n",
      "    ecaps = calculate_search_metrics(ecaps)\n",
      "\n",
      "    return ecaps\n",
      "\n",
      "def main(mb_search_data, completed_sample_df):\n",
      "    ecaps = create_ecaps(mb_search_data, completed_sample_df)\n",
      "    ecaps_summary = summarize_ecaps(ecaps)\n",
      "    return ecaps_summary\n"
     ]
    }
   ],
   "source": [
    "print(final.merged_pyspark_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
